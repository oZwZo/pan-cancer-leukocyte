{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram,distance \n",
    "from sklearn.metrics import calinski_harabasz_score,fowlkes_mallows_score,silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from IPython.core.pylabtools import figsize\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ac.fit_predict(v_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'braycurtis', 'canberra', 'chebyshev', 'cityblock','correlation', 'cosine', 'dice', 'euclidean', 'hamming',\n",
    "    'jaccard', 'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n",
    "    'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
    "    'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMat=distance.pdist(v_data,metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=linkage(disMat,method='average') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd90b813f61c45b8afa3142975fad751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "tree = dendrogram(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=5)\n",
    "data = np.random.random((3149,22))\n",
    "pred = ac.fit_predict(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ls = [AgglomerativeClustering(n_clusters=i).fit_predict(v_data) for i in range(5,20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics to evaulate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz index\n",
    "manunally compute SS_b and SS_w\n",
    "\n",
    "$$ CH = \\frac{SS_B}{SS_W} \\times \\frac{N-k}{k-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.ones((2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_metrics as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_metrics' from '/home/ZwZ/script/HER2_prediction/my_metrics.py'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### silhouette coefficience\n",
    "\n",
    "$$S_i = \\frac{\\beta_i - \\alpha_i}{\\max(\\alpha_i,\\beta_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s my_silhouette my_metrics.py\n",
    "def my_silhouette(X,label,metric='euclidean'):\n",
    "    r\"\"\"math:`S_i = \\frac{\\beta_i - \\alpha_i}{\\max(\\alpha_i,\\beta_i)}`\n",
    "    \"\"\"\n",
    "    \n",
    "    S = np.zeros(X.shape[0])\n",
    "    alpha = np.zeros(X.shape[0])\n",
    "    beta = np.zeros(X.shape[0])\n",
    "    cluster_data = {}\n",
    "    for cluster in np.unique(label):\n",
    "         cluster_data[cluster] = X[label == cluster]            # ie. (100,22)\n",
    "        \n",
    "    for cluster in np.unique(label):\n",
    "        c_index = np.where(label == cluster)[0]\n",
    "        # ----------- compute a_i of the current class ------------\n",
    "        X_c = cluster_data[cluster]\n",
    "        alpha_dist = pairwise.pairwise_distances(X_c)\n",
    "        #alpha = np.mean(alpha_dist,axis=1)\n",
    "        alpha[c_index] = np.sum(alpha_dist,axis=1)/(alpha_dist.shape[1]-1)\n",
    "        \n",
    "        # ----------- compute b_i of the current class ------------        \n",
    "        beta_ls = []\n",
    "        for other in np.unique(label):\n",
    "            if cluster == other:\n",
    "                continue\n",
    "            X_nc = cluster_data[other]                           # ie. (300,22)\n",
    "            beta_dist = pairwise.pairwise_distances(X_c,X_nc)    # ie. (100,300)\n",
    "            beta_ls.append(np.mean(beta_dist,axis=1))\n",
    "        beta[c_index] = np.min(np.stack(beta_ls),axis=0)\n",
    "        \n",
    "        #x_nc = X[label != cluster]\n",
    "        #beta_dist = pairwise.pairwise_distances(X_c,X_nc)\n",
    "        #beta = np.mean(beta_dist,axis=1)\n",
    "        \n",
    "        # ----------- compute S_i of the current class ------------\n",
    "                \n",
    "        S[c_index] = (beta[c_index] - alpha[c_index])/np.maximum(beta[c_index],alpha[c_index])\n",
    "\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_silhouette(v_dat.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_samples,intra_clust_dists,inter_clust_dists = silhouette_samples(v_data,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jump score\n",
    "$$d_k = \\frac{1}{p} \\min_{c_1,...,c_k} E[(X-C_x)^T\\Sigma^{-1}(X-C_x)]$$\n",
    "$$J_k = d_k-d_{k-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_of_k(X,labels):\n",
    "    p = X.shape[1]\n",
    "    Centoid = np.stack([np.mean(X[labels == cluster],axis=0) for cluster in np.unique(labels)])\n",
    "    dist_M = pairwise.pairwise_distances(X,Centoid,metric='mahalanobis')\n",
    "    dk =  np.min(np.mean(dist_M,axis=0))/p\n",
    "    return dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap statistics\n",
    "\n",
    "$$ Gap(k)=\\frac{1}{B} \\sum_{b=1}^B \\log(W_{kb}^*) - \\log(W_k) $$ \n",
    "\n",
    "*note: the cluster center of random datas should be determined by itself with k-means\n",
    "\n",
    "$$ w' = \\frac{1}{B} \\sum_{b=1}^B \\log(W_{kb}^*) $$\n",
    "\n",
    "$$ sd(k) = \\sqrt{ \\frac{1}{B} \\sum_{b} (\\log(W_{kb}^*) - w')^2} $$\n",
    "\n",
    "$$ s_k = \\sqrt{\\frac{1+B}{B} sd(k)} $$\n",
    "\n",
    "$$ k = \\min k $$\n",
    "$$s.t.   Gap(k) >= Gap(k+1) +s_{k+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_statistics(X,labels,n_random=100):\n",
    "    r\"\"\" compute the Gap(k) and sk for current data with label\n",
    "    refer: \n",
    "    ... https://blog.csdn.net/baidu_17640849/article/details/70769555?fps=1&locationNum=2\n",
    "    \"\"\"\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    \n",
    "    # compute Wk\n",
    "    Wk =  compute_wk(X,labels)\n",
    "    \n",
    "    # random sample and compute Wkb\n",
    "    W_prims = []\n",
    "    data_range = np.matrix(np.diag(X.max(axis=0) - X.min(axis=0)))\n",
    "\n",
    "    for r in range(n_random):\n",
    "        refs = np.random.random(X.shape)*data_range + X.min(axis=0)    # ie. (3194,22)\n",
    "        refs_center,refs_label = kmeans2(refs,n_cluster)        # ie. (5,22) ,(3194,)\n",
    "        refs_center = refs_center.take(refs_label,axis=0)              # ie. (3194,22)\n",
    "        Wkb = pairwise.paired_distances(refs,refs_center)    #  sum ( (3194,) ) -> R\n",
    "        assert(Wkb.shape == (X.shape[0],))\n",
    "        W_prims.append(np.log(np.sum(Wkb)))\n",
    "    \n",
    "    W_prim = np.mean(W_prims)\n",
    "    \n",
    "    # compute sd_k\n",
    "    sd_k = np.sqrt(np.mean((np.array(W_prims) - W_prim)**2))\n",
    "    s_k = np.sqrt((1+n_random)*sd_k/n_random )\n",
    "    \n",
    "    # compute Gap(k)\n",
    "    Gap_k = W_prim - np.log(Wk)\n",
    "    return Gap_k , s_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0323663849298308, 0.055648033901140696)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_statistics(v_data,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.cluster.vq\n",
    "import scipy.spatial.distance\n",
    "import numpy as np\n",
    "EuclDist = scipy.spatial.distance.euclidean\n",
    "def gapStat(data, resf=None, nrefs=10, ks=range(1,10)):\n",
    "    '''\n",
    "    Gap statistics\n",
    "    '''\n",
    "    # MC\n",
    "    shape = data.shape\n",
    "    if resf == None:\n",
    "        x_max = data.max(axis=0)\n",
    "        x_min = data.min(axis=0)\n",
    "        dists = np.matrix(np.diag(x_max-x_min))\n",
    "        rands = np.random.random_sample(size=(shape[0], shape[1], nrefs))\n",
    "        for i in range(nrefs):\n",
    "            rands[:,:,i] = rands[:,:,i]*dists+x_min\n",
    "    else:\n",
    "        rands = refs\n",
    "    gaps = np.zeros((len(ks),))\n",
    "    gapDiff = np.zeros(len(ks)-1,)\n",
    "    sdk = np.zeros(len(ks),)\n",
    "    for (i,k) in enumerate(ks):\n",
    "        (cluster_mean, cluster_res) = scipy.cluster.vq.kmeans2(data, k)\n",
    "        Wk = sum([EuclDist(data[m,:], cluster_mean[cluster_res[m],:]) for m in range(shape[0])])\n",
    "        WkRef = np.zeros((rands.shape[2],))\n",
    "        for j in range(rands.shape[2]):\n",
    "            (kmc,kml) = scipy.cluster.vq.kmeans2(rands[:,:,j], k)\n",
    "            WkRef[j] = sum([EuclDist(rands[m,:,j],kmc[kml[m],:]) for m in range(shape[0])])\n",
    "        gaps[i] = scipy.log(scipy.mean(WkRef))-scipy.log(Wk)\n",
    "        sdk[i] = np.sqrt((1.0+nrefs)/nrefs)*np.std(scipy.log(WkRef))\n",
    "\n",
    "        if i > 0:\n",
    "            gapDiff[i-1] = gaps[i-1] - gaps[i] + sdk[i]\n",
    "    return gaps, gapDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ZwZ/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: scipy.mean is deprecated and will be removed in SciPy 2.0.0, use numpy.mean instead\n",
      "/home/ZwZ/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "/home/ZwZ/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.89662957, 0.96167973, 1.02684054, 1.0582979 , 1.07790736,\n",
       "        1.08962838, 1.10040198, 1.12412361, 1.1318255 ]),\n",
       " array([-0.06343369, -0.06293012, -0.02724294, -0.01728405, -0.00974109,\n",
       "        -0.0084144 , -0.02164338, -0.00517767]))"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gapStat(v_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB method\n",
    "<font size=4>\n",
    "$$ DB = \\frac{1}{k} \\sum_{k=1}^k R_i$$\n",
    "$$ R_i = \\max_{j=1...k,i \\neq j} R_{ij}$$\n",
    "$$ R_{ij} = \\frac{s_i + s_j}{d_{ij}} $$\n",
    "$$ d_{ij} = d(v_i,v_j)$$</font>\n",
    "where $v_i$ is the cluster center for class $i$  \n",
    "<font size=4>\n",
    "$$ s_i = \\frac{1}{||c_i||} \\sum_{x \\in c_i} d(x,v_i)$$ </font>\n",
    "*note $||c_i||$ is scale by the size of class $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DB_score(X,labels):\n",
    "    \n",
    "    n_cluster = len(np.unique(labels))\n",
    "    # get v\n",
    "    v = np.stack([np.mean(X[labels==c],axis=0) for c in np.unique(labels)])\n",
    "\n",
    "    # d_ij\n",
    "    d_ij = pairwise.pairwise_distances(v)\n",
    "    for i in range(d_ij.shape[0]):\n",
    "        d_ij[i,i] = np.inf\n",
    "\n",
    "    # compute si\n",
    "    s_i = np.stack([np.mean(pairwise.pairwise_distances(X[labels ==c],v[c].reshape(1,-1))) for c in np.unique(labels)])\n",
    "    s_i_bc = np.broadcast_to(s_i,(n_cluster,n_cluster))\n",
    "\n",
    "    # compute R\n",
    "    R_ij = np.divide((s_i_bc.T + s_i_bc),d_ij)\n",
    "    R_i = np.max(R_ij,axis=1)\n",
    "\n",
    "    # compute DB\n",
    "    DB = np.mean(R_i)\n",
    "\n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Krzanowski & Lai \n",
    "$$ DIFF(k) = (k-1)^{2/p}W_{k-1} - k^{2/p}W_k$$  \n",
    "&nbsp;\n",
    "$$ KL(k) = |\\frac{DIFF(k)}{DIFF(k+1} |$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wk(X,labels,KL=False):\n",
    "    \"\"\"\n",
    "    Wk stands for the sum of euclidean distance of data points to its cooresponding cluster center\n",
    "    arguments:\n",
    "    ...X : (n_samples,n_feature)\n",
    "    ...labels : (n_samples,) dtype = int\n",
    "    ...KL : recale by k and data dimension p , should set to 'True' when later used to \n",
    "    \"\"\"\n",
    "    p = X.shape[1]\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    # get v\n",
    "    v = np.stack([np.mean(X[labels==c],axis=0) for c in np.unique(labels)])\n",
    "\n",
    "    # compute si\n",
    "    wk = np.sum([np.sum(pairwise.pairwise_distances(X[labels ==c],v[c].reshape(1,-1))) for c in np.unique(labels)])\n",
    "    ## \n",
    "    if KL:\n",
    "        wk = (n_cluster)**(2/p)*wk\n",
    "    \n",
    "    return wk\n",
    "\n",
    "def KL_score(X,label_ls):\n",
    "    '''\n",
    "    when given data X and a list containing cluster labels with different n_clusters setting\n",
    "    argument:\n",
    "    ...X : (n_samples,n_feature)\n",
    "    ...label_ls : list \n",
    "    outpur:\n",
    "    ... KL_k : list , length : len(label_ls) - 2\n",
    "    '''\n",
    "    k_ls = [len(np.unique(labels)) for labels in label_ls]\n",
    "    wk_ls = [compute_wk(X,labels,KL=True) for labels in label_ls]\n",
    "    \n",
    "    # len : ks -1\n",
    "    DIFF_k = [wk_ls[i-1] - wk_ls[i] for i in range(1,len(label_ls))]\n",
    "    #\n",
    "    KL_k = [np.abs(np.divide(DIFF_k[i],DIFF_k[i+1])) for i in range(0,len(DIFF_k)-1)]\n",
    "    return KL_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3758308621657853,\n",
       " 0.6831266543112948,\n",
       " 0.740430633655721,\n",
       " 1.199114559438958,\n",
       " 2.7226158423891142,\n",
       " 1.1037197644914154,\n",
       " 0.8279054265084943,\n",
       " 0.8183576685961351,\n",
       " 0.8906785997216118,\n",
       " 1.1487437766258237,\n",
       " 1.2454179777800887,\n",
       " 1.242924589671839,\n",
       " 1.6712855287659611]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL_score(v_data,label_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hartigan method\n",
    "$$H(k) = (\\frac{W_k}{W_{k+1}} -1)(n-k-1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hartigan_score(X,label_ls):\n",
    "    \"\"\"\n",
    "    the calculation of Hartigan score  require Wk and Wk+1 \n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    k_ls = [len(np.unique(labels)) for labels in label_ls]\n",
    "    \n",
    "    # remember to set KL=True , for Wk share the same definition with that in KL method\n",
    "    wk_ls = [compute_wk(X,labels,KL=True) for labels in label_ls]\n",
    "    \n",
    "    # compute Hk , require W_{k} and W_{k+1}\n",
    "    Hk_ls = [(wk_ls[i]/wk_ls[i+1] -1)*(n-k_ls[i]-1) for i in range(0,len(k_ls)-1)]\n",
    "    \n",
    "    return Hk_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.347179913744952,\n",
       " 17.790886085023395,\n",
       " 26.25263334590904,\n",
       " 35.84928033494645,\n",
       " 30.174232593246618,\n",
       " 11.11853477665086,\n",
       " 10.10291607509216,\n",
       " 12.246733260964845,\n",
       " 15.031973810180697,\n",
       " 16.96292495616799,\n",
       " 14.831670806761027,\n",
       " 11.950615340324068,\n",
       " 9.641444191517095,\n",
       " 5.777682017484094]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hartigan_score(v_data,label_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL in one combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(my_metrics)\n",
    "\n",
    "from my_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = cluster_metrics(v_data,label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
